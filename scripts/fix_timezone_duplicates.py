#!/usr/bin/env python3
"""
ä¿®å¤æ—¶åŒºé—®é¢˜å¯¼è‡´çš„é‡å¤æ•°æ®

è¯¥è„šæœ¬ä¼šï¼š
1. å¤‡ä»½å½“å‰æ•°æ®åº“
2. åˆ é™¤APIåŒæ­¥çš„é‡å¤æ•°æ®
3. é‡æ–°ä½¿ç”¨ä¿®å¤åçš„æ—¶åŒºå¤„ç†é€»è¾‘å¯¼å…¥Excelæ•°æ®
4. éªŒè¯æ•°æ®ä¸€è‡´æ€§
"""

import sys
import os
import shutil
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import database_setup
import journal_core
import utilities


def backup_database():
    """å¤‡ä»½å½“å‰æ•°æ®åº“"""
    backup_name = f"data/trading_journal_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}.db"
    if os.path.exists(database_setup.DB_PATH):
        shutil.copy2(database_setup.DB_PATH, backup_name)
        print(f"âœ… æ•°æ®åº“å·²å¤‡ä»½åˆ°: {backup_name}")
        return backup_name
    else:
        print("âš ï¸  æ•°æ®åº“æ–‡ä»¶ä¸å­˜åœ¨ï¼Œæ— éœ€å¤‡ä»½")
        return None


def analyze_duplicates():
    """åˆ†æé‡å¤æ•°æ®æƒ…å†µ"""
    print("\n=== åˆ†æé‡å¤æ•°æ®æƒ…å†µ ===")
    
    all_trades = database_setup.get_all_trades()
    excel_trades = [t for t in all_trades if t.get('data_source') == 'excel']
    api_trades = [t for t in all_trades if t.get('data_source', '').startswith('binance_api')]
    
    print(f"Excelè®°å½•: {len(excel_trades)} æ¡")
    print(f"APIè®°å½•: {len(api_trades)} æ¡")
    print(f"æ€»è®°å½•: {len(all_trades)} æ¡")
    
    # æŸ¥æ‰¾å¯èƒ½çš„é‡å¤è®°å½•ï¼ˆç›¸åŒçš„ä»·æ ¼ã€æ•°é‡ã€äº¤æ˜“å¯¹ï¼‰
    potential_duplicates = []
    for api_trade in api_trades:
        for excel_trade in excel_trades:
            if (api_trade['symbol'] == excel_trade['symbol'] and
                api_trade['side'] == excel_trade['side'] and
                abs(float(api_trade['price']) - float(excel_trade['price'])) < 0.0001 and
                abs(float(api_trade['quantity']) - float(excel_trade['quantity'])) < 0.000001):
                
                potential_duplicates.append({
                    'api_trade': api_trade,
                    'excel_trade': excel_trade
                })
    
    print(f"\nå‘ç° {len(potential_duplicates)} å¯¹å¯èƒ½çš„é‡å¤è®°å½•:")
    for i, dup in enumerate(potential_duplicates, 1):
        api_t = dup['api_trade']
        excel_t = dup['excel_trade']
        print(f"{i}. {api_t['symbol']} | API: {api_t['utc_time']} | Excel: {excel_t['utc_time']}")
    
    return potential_duplicates


def clean_api_duplicates():
    """åˆ é™¤APIåŒæ­¥çš„é‡å¤æ•°æ®"""
    print("\n=== æ¸…ç†APIé‡å¤æ•°æ® ===")
    
    conn = database_setup.sqlite3.connect(database_setup.DB_PATH)
    cursor = conn.cursor()
    
    # åˆ é™¤æ‰€æœ‰APIåŒæ­¥çš„æ•°æ®
    cursor.execute("DELETE FROM trades WHERE data_source LIKE 'binance_api%'")
    deleted_count = cursor.rowcount
    
    conn.commit()
    conn.close()
    
    print(f"âœ… å·²åˆ é™¤ {deleted_count} æ¡APIåŒæ­¥çš„é‡å¤è®°å½•")
    return deleted_count


def verify_excel_data():
    """éªŒè¯Excelæ•°æ®å®Œæ•´æ€§"""
    print("\n=== éªŒè¯Excelæ•°æ®å®Œæ•´æ€§ ===")
    
    all_trades = database_setup.get_all_trades()
    excel_trades = [t for t in all_trades if t.get('data_source') == 'excel']
    
    print(f"å‰©ä½™Excelè®°å½•: {len(excel_trades)} æ¡")
    
    # æ˜¾ç¤ºæœ€è¿‘å‡ æ¡è®°å½•çš„æ—¶é—´
    if excel_trades:
        recent_trades = sorted(excel_trades, key=lambda x: x['utc_time'])[-5:]
        print("æœ€è¿‘5æ¡Excelè®°å½•çš„æ—¶é—´:")
        for trade in recent_trades:
            print(f"  {trade['utc_time']} | {trade['symbol']} | {trade['side']}")
    
    return len(excel_trades)


def test_time_conversion():
    """æµ‹è¯•æ—¶é—´è½¬æ¢åŠŸèƒ½"""
    print("\n=== æµ‹è¯•æ—¶é—´è½¬æ¢åŠŸèƒ½ ===")
    
    # æµ‹è¯•æ ·ä¾‹
    test_cases = [
        "2025-06-21 18:56:39",  # CSTæ—¶é—´
        "2025-06-21 21:31:54",  # CSTæ—¶é—´
    ]
    
    for cst_time in test_cases:
        utc_time = utilities.normalize_excel_time_to_utc(cst_time)
        print(f"CST: {cst_time} -> UTC: {utc_time}")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ”§ ä¿®å¤æ—¶åŒºé—®é¢˜å¯¼è‡´çš„é‡å¤æ•°æ®")
    print("=" * 50)
    
    try:
        # 1. å¤‡ä»½æ•°æ®åº“
        backup_file = backup_database()
        
        # 2. åˆ†æé‡å¤æƒ…å†µ
        duplicates = analyze_duplicates()
        
        if not duplicates:
            print("âœ… æœªå‘ç°é‡å¤æ•°æ®ï¼Œæ— éœ€ä¿®å¤")
            return
        
        # 3. è¯¢é—®ç”¨æˆ·æ˜¯å¦ç»§ç»­
        response = input(f"\nå‘ç° {len(duplicates)} å¯¹é‡å¤è®°å½•ï¼Œæ˜¯å¦ç»§ç»­ä¿®å¤ï¼Ÿ(y/N): ")
        if response.lower() != 'y':
            print("âŒ ç”¨æˆ·å–æ¶ˆæ“ä½œ")
            return
        
        # 4. æ¸…ç†APIé‡å¤æ•°æ®
        deleted_count = clean_api_duplicates()
        
        # 5. éªŒè¯Excelæ•°æ®
        excel_count = verify_excel_data()
        
        # 6. æµ‹è¯•æ—¶é—´è½¬æ¢
        test_time_conversion()
        
        # 7. é‡æ–°è®¡ç®—PnL
        print("\n=== é‡æ–°è®¡ç®—ç›ˆäº ===")
        journal_core.update_all_pnl()
        
        print("\nâœ… ä¿®å¤å®Œæˆï¼")
        print(f"ğŸ“Š å½“å‰æ•°æ®åº“è®°å½•æ•°: {database_setup.get_total_trade_count()}")
        print(f"ğŸ’¾ å¤‡ä»½æ–‡ä»¶: {backup_file}")
        
        print("\nğŸ’¡ å»ºè®®ï¼š")
        print("1. ä½¿ç”¨ 'python main.py report summary' æŸ¥çœ‹ä¿®å¤åçš„ç»Ÿè®¡")
        print("2. å¦‚æœéœ€è¦åŒæ­¥æœ€æ–°æ•°æ®ï¼Œä½¿ç”¨ 'python main.py api sync --days 7'")
        
    except Exception as e:
        print(f"âŒ ä¿®å¤è¿‡ç¨‹å‡ºé”™: {e}")
        if 'backup_file' in locals() and backup_file:
            print(f"ğŸ’¡ å¯ä»¥ä»å¤‡ä»½æ¢å¤: cp {backup_file} {database_setup.DB_PATH}")


if __name__ == "__main__":
    main() 